<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils.py" />
              <option name="originalContent" value="import numpy as np&#10;import torch&#10;import os&#10;import h5py&#10;import pickle&#10;import fnmatch&#10;import cv2&#10;from time import time&#10;&#10;from pyasn1_modules.rfc4357 import id_Gost28147_89_CryptoPro_RIC_1_ParamSet&#10;from torch.utils.data import TensorDataset, DataLoader&#10;import torchvision.transforms as transforms&#10;&#10;import IPython&#10;e = IPython.embed&#10;&#10;def flatten_list(l):&#10;    return [item for sublist in l for item in sublist]&#10;&#10;class EpisodicDataset(torch.utils.data.Dataset):&#10;    def __init__(self, dataset_path_list, camera_names, norm_stats, episode_ids, episode_len, chunk_size, policy_class):&#10;        super(EpisodicDataset).__init__()&#10;        self.episode_ids = episode_ids&#10;        self.dataset_path_list = dataset_path_list&#10;        self.camera_names = camera_names&#10;        self.norm_stats = norm_stats&#10;        self.episode_len = episode_len&#10;        self.chunk_size = chunk_size&#10;        self.cumulative_len = np.cumsum(self.episode_len)&#10;        self.max_episode_len = max(episode_len)&#10;        self.policy_class = policy_class&#10;        if self.policy_class == 'Diffusion':&#10;            self.augment_images = True&#10;        else:&#10;            self.augment_images = False&#10;        self.transformations = None&#10;        self.__getitem__(0) # initialize self.is_sim and self.transformations&#10;        self.is_sim = False&#10;&#10;    # def __len__(self):&#10;    #     return sum(self.episode_len)&#10;&#10;    def _locate_transition(self, index):&#10;        assert index &lt; self.cumulative_len[-1]&#10;        episode_index = np.argmax(self.cumulative_len &gt; index) # argmax returns first True index&#10;        start_ts = index - (self.cumulative_len[episode_index] - self.episode_len[episode_index])&#10;        episode_id = self.episode_ids[episode_index]&#10;        return episode_id, start_ts&#10;&#10;&#10;&#10;    # utils.pyang&#10;&#10;    def __getitem__(self, index):&#10;        episode_id, start_ts = self._locate_transition(index)&#10;        dataset_path = self.dataset_path_list[episode_id]&#10;        with h5py.File(dataset_path, 'r') as root:&#10;            try:  # some legacy data does not have this attribute&#10;                is_sim = root.attrs['sim']&#10;            except:&#10;                is_sim = False&#10;            compressed = root.attrs.get('compress', False)&#10;&#10;            action = root['/action'][()]&#10;            original_action_shape = action.shape&#10;            episode_len = original_action_shape[0]&#10;            # get observation at start_ts only&#10;            qpos = root['/observations/qpos'][start_ts]&#10;            qvel = root['/observations/qvel'][start_ts]&#10;            image_dict = dict()&#10;            for cam_name in self.camera_names:&#10;                image_dict[cam_name] = root[f'/observations/images/{cam_name}'][start_ts]&#10;&#10;            if compressed:&#10;                for cam_name in image_dict.keys():&#10;                    decompressed_image = cv2.imdecode(image_dict[cam_name], 1)&#10;                    image_dict[cam_name] = np.array(decompressed_image)&#10;&#10;            # get all actions after and including start_ts&#10;            if is_sim:&#10;                action = action[start_ts:]&#10;            else:&#10;                action = action[max(0, start_ts - 1):]  # hack, to make timesteps more aligned&#10;&#10;        # self.is_sim = is_sim&#10;&#10;        # --- 修改开始 ---&#10;        # 确保动作序列长度固定为 chunk_size&#10;        action_len = action.shape[0]&#10;        padded_action = np.zeros((self.chunk_size, original_action_shape[1]), dtype=np.float32)&#10;        # 截断或填充动作序列&#10;        copy_len = min(action_len, self.chunk_size)&#10;        padded_action[:copy_len] = action[:copy_len]&#10;&#10;        # 创建 is_pad 掩码&#10;        is_pad = np.zeros(self.chunk_size, dtype=bool)&#10;        is_pad[copy_len:] = True&#10;        # --- 修改结束 ---&#10;&#10;        # new axis for different cameras&#10;        all_cam_images = []&#10;        for cam_name in self.camera_names:&#10;            all_cam_images.append(image_dict[cam_name])&#10;        all_cam_images = np.stack(all_cam_images, axis=0)&#10;&#10;        # construct observations&#10;        image_data = torch.from_numpy(all_cam_images)&#10;        qpos_data = torch.from_numpy(qpos).float()&#10;        action_data = torch.from_numpy(padded_action).float()&#10;        is_pad = torch.from_numpy(is_pad).bool()&#10;&#10;        # channel last&#10;        image_data = torch.einsum('k h w c -&gt; k c h w', image_data)&#10;&#10;        # augmentation&#10;        if self.transformations is None:&#10;            print('Initializing transformations')&#10;            original_size = image_data.shape[2:]&#10;            ratio = 0.95&#10;            self.transformations = [&#10;                transforms.RandomCrop(size=[int(original_size[0] * ratio), int(original_size[1] * ratio)]),&#10;                transforms.Resize(original_size, antialias=True),&#10;                transforms.RandomRotation(degrees=[-5.0, 5.0], expand=False),&#10;                transforms.ColorJitter(brightness=0.3, contrast=0.4, saturation=0.5)  # , hue=0.08)&#10;            ]&#10;&#10;        if self.augment_images:&#10;            for transform in self.transformations:&#10;                image_data = transform(image_data)&#10;&#10;        # normalize image and change dtype to float&#10;        image_data = image_data / 255.0&#10;&#10;        if self.policy_class == 'Diffusion':&#10;            # normalize to [-1, 1]&#10;            action_data = ((action_data - self.norm_stats[&quot;action_min&quot;]) / (&#10;                        self.norm_stats[&quot;action_max&quot;] - self.norm_stats[&quot;action_min&quot;])) * 2 - 1&#10;        else:&#10;            # normalize to mean 0 std 1&#10;            action_data = (action_data - self.norm_stats[&quot;action_mean&quot;]) / self.norm_stats[&quot;action_std&quot;]&#10;&#10;        qpos_data = (qpos_data - self.norm_stats[&quot;qpos_mean&quot;]) / self.norm_stats[&quot;qpos_std&quot;]&#10;&#10;        return image_data, qpos_data, action_data, is_pad&#10;&#10;&#10;def get_norm_stats(dataset_path_list):&#10;    all_qpos_data = []&#10;    all_action_data = []&#10;    all_episode_len = []&#10;&#10;    for dataset_path in dataset_path_list:&#10;        try:&#10;            with h5py.File(dataset_path, 'r') as root:&#10;                qpos = root['/observations/qpos'][()]&#10;                qvel = root['/observations/qvel'][()]&#10;                if '/base_action' in root:&#10;                    base_action = root['/base_action'][()]&#10;                    base_action = preprocess_base_action(base_action)&#10;                    action = np.concatenate([root['/action'][()], base_action], axis=-1)&#10;                else:&#10;                    action = root['/action'][()]&#10;        except Exception as e:&#10;            print(f'Error loading {dataset_path} in get_norm_stats')&#10;            print(e)&#10;            quit()&#10;        all_qpos_data.append(torch.from_numpy(qpos))&#10;        all_action_data.append(torch.from_numpy(action))&#10;        all_episode_len.append(len(qpos))&#10;    all_qpos_data = torch.cat(all_qpos_data, dim=0)&#10;    all_action_data = torch.cat(all_action_data, dim=0)&#10;&#10;    # normalize action data&#10;    action_mean = all_action_data.mean(dim=[0]).float()&#10;    action_std = all_action_data.std(dim=[0]).float()&#10;    action_std = torch.clip(action_std, 1e-2, np.inf) # clipping&#10;&#10;    # normalize qpos data&#10;    qpos_mean = all_qpos_data.mean(dim=[0]).float()&#10;    qpos_std = all_qpos_data.std(dim=[0]).float()&#10;    qpos_std = torch.clip(qpos_std, 1e-2, np.inf) # clipping&#10;&#10;    action_min = all_action_data.min(dim=0).values.float()&#10;    action_max = all_action_data.max(dim=0).values.float()&#10;&#10;    eps = 0.0001&#10;    stats = {&quot;action_mean&quot;: action_mean.numpy(), &quot;action_std&quot;: action_std.numpy(),&#10;             &quot;action_min&quot;: action_min.numpy() - eps,&quot;action_max&quot;: action_max.numpy() + eps,&#10;             &quot;qpos_mean&quot;: qpos_mean.numpy(), &quot;qpos_std&quot;: qpos_std.numpy(),&#10;             &quot;example_qpos&quot;: qpos}&#10;&#10;    return stats, all_episode_len&#10;&#10;def find_all_hdf5(dataset_dir, skip_mirrored_data):&#10;    hdf5_files = []&#10;    for root, dirs, files in os.walk(dataset_dir):&#10;        for filename in fnmatch.filter(files, '*.hdf5'):&#10;            if 'features' in filename: continue&#10;            if skip_mirrored_data and 'mirror' in filename:&#10;                continue&#10;            hdf5_files.append(os.path.join(root, filename))&#10;    print(f'Found {len(hdf5_files)} hdf5 files')&#10;    return hdf5_files&#10;&#10;def BatchSampler(batch_size, episode_len_l, sample_weights):&#10;    sample_probs = np.array(sample_weights) / np.sum(sample_weights) if sample_weights is not None else None&#10;    sum_dataset_len_l = np.cumsum([0] + [np.sum(episode_len) for episode_len in episode_len_l])&#10;    while True:&#10;        batch = []&#10;        for _ in range(batch_size):&#10;            episode_idx = np.random.choice(len(episode_len_l), p=sample_probs)&#10;            step_idx = np.random.randint(sum_dataset_len_l[episode_idx], sum_dataset_len_l[episode_idx + 1])&#10;            batch.append(step_idx)&#10;        yield batch&#10;&#10;def load_data(dataset_dir_l, name_filter, camera_names, batch_size_train, batch_size_val, chunk_size, skip_mirrored_data=False,&#10;              load_pretrain=False, policy_class=None, stats_dir_l=None, sample_weights=None, train_ratio=0.99):&#10;&#10;    if type(dataset_dir_l) == str:&#10;        dataset_dir_l = [dataset_dir_l]&#10;    dataset_path_list_list = [find_all_hdf5(dataset_dir, skip_mirrored_data) for dataset_dir in dataset_dir_l]&#10;    num_episodes_0 = len(dataset_path_list_list[0])&#10;    dataset_path_list = flatten_list(dataset_path_list_list)&#10;    dataset_path_list = [n for n in dataset_path_list if name_filter(n)]&#10;    num_episodes_l = [len(dataset_path_list) for dataset_path_list in dataset_path_list_list]&#10;    num_episodes_cumsum = np.cumsum(num_episodes_l)&#10;&#10;    # obtain train test split on dataset_dir_l[0]&#10;    shuffled_episode_ids_0 = np.random.permutation(num_episodes_0)&#10;    train_episode_ids_0 = shuffled_episode_ids_0[:int(train_ratio * num_episodes_0)]#####注意&#10;    val_episode_ids_0 = shuffled_episode_ids_0[int(train_ratio * num_episodes_0):]&#10;    train_episode_ids_l = [train_episode_ids_0] + [np.arange(num_episodes) + num_episodes_cumsum[idx] for idx, num_episodes in enumerate(num_episodes_l[1:])]&#10;    val_episode_ids_l = [val_episode_ids_0]&#10;    train_episode_ids = np.concatenate(train_episode_ids_l)&#10;    val_episode_ids = np.concatenate(val_episode_ids_l)&#10;    print(f'\n\nData from: {dataset_dir_l}\n- Train on {[len(x) for x in train_episode_ids_l]} episodes\n- Test on {[len(x) for x in val_episode_ids_l]} episodes\n\n')&#10;&#10;    # obtain normalization stats for qpos and action&#10;    # if load_pretrain:&#10;    #     with open(os.path.join('/home/zfu/interbotix_ws/src/act/ckpts/pretrain_all', 'dataset_stats.pkl'), 'rb') as f:&#10;    #         norm_stats = pickle.load(f)&#10;    #     print('Loaded pretrain dataset stats')&#10;    _, all_episode_len = get_norm_stats(dataset_path_list)&#10;    train_episode_len_l = [[all_episode_len[i] for i in train_episode_ids] for train_episode_ids in train_episode_ids_l]&#10;    val_episode_len_l = [[all_episode_len[i] for i in val_episode_ids] for val_episode_ids in val_episode_ids_l]&#10;    train_episode_len = flatten_list(train_episode_len_l)&#10;    val_episode_len = flatten_list(val_episode_len_l)&#10;    if stats_dir_l is None:&#10;        stats_dir_l = dataset_dir_l&#10;    elif type(stats_dir_l) == str:&#10;        stats_dir_l = [stats_dir_l]&#10;    norm_stats, _ = get_norm_stats(flatten_list([find_all_hdf5(stats_dir, skip_mirrored_data) for stats_dir in stats_dir_l]))&#10;    print(f'Norm stats from: {stats_dir_l}')&#10;&#10;    batch_sampler_train = BatchSampler(batch_size_train, train_episode_len_l, sample_weights)&#10;    batch_sampler_val = BatchSampler(batch_size_val, val_episode_len_l, None)&#10;    # print(f'train_episode_len: {train_episode_len}, val_episode_len: {val_episode_len}, train_episode_ids: {train_episode_ids}, val_episode_ids: {val_episode_ids}')&#10;&#10;    # construct dataset and dataloader&#10;    train_dataset = EpisodicDataset(dataset_path_list, camera_names, norm_stats, train_episode_ids, train_episode_len, chunk_size, policy_class)&#10;    val_dataset = EpisodicDataset(dataset_path_list, camera_names, norm_stats, val_episode_ids, val_episode_len, chunk_size, policy_class)&#10;&#10;    for data in train_dataset:&#10;        print(&quot;train_dataset_训练集张量&quot;, data[2].shape)&#10;        break&#10;    for data in val_dataset:&#10;        print(&quot;测试集是多少&quot;,data[2].shape)&#10;        break&#10;    # train_num_workers = (8 if os.getlogin() == 'zfu' else 16) if train_dataset.augment_images else 2&#10;    # val_num_workers = 8 if train_dataset.augment_images else 2&#10;    train_num_workers = 8&#10;    val_num_workers = 8&#10;    print(f'Augment images: {train_dataset.augment_images}, train_num_workers: {train_num_workers}, val_num_workers: {val_num_workers}')&#10;    train_dataloader = DataLoader(train_dataset, batch_sampler=batch_sampler_train, pin_memory=True, num_workers=train_num_workers, prefetch_factor=2)&#10;    val_dataloader = DataLoader(val_dataset, batch_sampler=batch_sampler_val, pin_memory=True, num_workers=val_num_workers, prefetch_factor=2)&#10;&#10;    return train_dataloader, val_dataloader, norm_stats, train_dataset.is_sim&#10;&#10;def calibrate_linear_vel(base_action, c=None):&#10;    if c is None:&#10;        c = 0.0 # 0.19&#10;    v = base_action[..., 0]&#10;    w = base_action[..., 1]&#10;    base_action = base_action.copy()&#10;    base_action[..., 0] = v - c * w&#10;    return base_action&#10;&#10;def smooth_base_action(base_action):&#10;    return np.stack([&#10;        np.convolve(base_action[:, i], np.ones(5)/5, mode='same') for i in range(base_action.shape[1])&#10;    ], axis=-1).astype(np.float32)&#10;&#10;def preprocess_base_action(base_action):&#10;    # base_action = calibrate_linear_vel(base_action)&#10;    base_action = smooth_base_action(base_action)&#10;&#10;    return base_action&#10;&#10;def postprocess_base_action(base_action):&#10;    linear_vel, angular_vel = base_action&#10;    linear_vel *= 1.0&#10;    angular_vel *= 1.0&#10;    # angular_vel = 0&#10;    # if np.abs(linear_vel) &lt; 0.05:&#10;    #     linear_vel = 0&#10;    return np.array([linear_vel, angular_vel])&#10;&#10;### env utils&#10;&#10;def sample_box_pose():&#10;    x_range = [0.0, 0.2]&#10;    y_range = [0.4, 0.6]&#10;    z_range = [0.05, 0.05]&#10;&#10;    ranges = np.vstack([x_range, y_range, z_range])&#10;    cube_position = np.random.uniform(ranges[:, 0], ranges[:, 1])&#10;&#10;    cube_quat = np.array([1, 0, 0, 0])&#10;    return np.concatenate([cube_position, cube_quat])&#10;&#10;def sample_insertion_pose():&#10;    # Peg&#10;    x_range = [0.1, 0.2]&#10;    y_range = [0.4, 0.6]&#10;    z_range = [0.05, 0.05]&#10;&#10;    ranges = np.vstack([x_range, y_range, z_range])&#10;    peg_position = np.random.uniform(ranges[:, 0], ranges[:, 1])&#10;&#10;    peg_quat = np.array([1, 0, 0, 0])&#10;    peg_pose = np.concatenate([peg_position, peg_quat])&#10;&#10;    # Socket&#10;    x_range = [-0.2, -0.1]&#10;    y_range = [0.4, 0.6]&#10;    z_range = [0.05, 0.05]&#10;&#10;    ranges = np.vstack([x_range, y_range, z_range])&#10;    socket_position = np.random.uniform(ranges[:, 0], ranges[:, 1])&#10;&#10;    socket_quat = np.array([1, 0, 0, 0])&#10;    socket_pose = np.concatenate([socket_position, socket_quat])&#10;&#10;    return peg_pose, socket_pose&#10;&#10;### helper functions&#10;&#10;def compute_dict_mean(epoch_dicts):&#10;    result = {k: None for k in epoch_dicts[0]}&#10;    num_items = len(epoch_dicts)&#10;    for k in result:&#10;        value_sum = 0&#10;        for epoch_dict in epoch_dicts:&#10;            value_sum += epoch_dict[k]&#10;        result[k] = value_sum / num_items&#10;    return result&#10;&#10;def detach_dict(d):&#10;    new_d = dict()&#10;    for k, v in d.items():&#10;        new_d[k] = v.detach()&#10;    return new_d&#10;&#10;def set_seed(seed):&#10;    torch.manual_seed(seed)&#10;    np.random.seed(seed)&#10;" />
              <option name="updatedContent" value="import numpy as np&#10;import torch&#10;import os&#10;import h5py&#10;import pickle&#10;import fnmatch&#10;import cv2&#10;from time import time&#10;&#10;from pyasn1_modules.rfc4357 import id_Gost28147_89_CryptoPro_RIC_1_ParamSet&#10;from torch.utils.data import TensorDataset, DataLoader&#10;import torchvision.transforms as transforms&#10;&#10;import IPython&#10;e = IPython.embed&#10;&#10;def flatten_list(l):&#10;    return [item for sublist in l for item in sublist]&#10;&#10;class EpisodicDataset(torch.utils.data.Dataset):&#10;    def __init__(self, dataset_path_list, camera_names, norm_stats, episode_ids, episode_len, chunk_size, policy_class):&#10;        super(EpisodicDataset).__init__()&#10;        self.episode_ids = episode_ids&#10;        self.dataset_path_list = dataset_path_list&#10;        self.camera_names = camera_names&#10;        self.norm_stats = norm_stats&#10;        self.episode_len = episode_len&#10;        self.chunk_size = chunk_size&#10;        self.cumulative_len = np.cumsum(self.episode_len)&#10;        self.max_episode_len = max(episode_len)&#10;        self.policy_class = policy_class&#10;        if self.policy_class == 'Diffusion':&#10;            self.augment_images = True&#10;        else:&#10;            self.augment_images = False&#10;        self.transformations = None&#10;        self.__getitem__(0) # initialize self.is_sim and self.transformations&#10;        self.is_sim = False&#10;&#10;    # def __len__(self):&#10;    #     return sum(self.episode_len)&#10;&#10;    def _locate_transition(self, index):&#10;        assert index &lt; self.cumulative_len[-1]&#10;        episode_index = np.argmax(self.cumulative_len &gt; index) # argmax returns first True index&#10;        start_ts = index - (self.cumulative_len[episode_index] - self.episode_len[episode_index])&#10;        episode_id = self.episode_ids[episode_index]&#10;        return episode_id, start_ts&#10;&#10;&#10;&#10;    # utils.pyang&#10;&#10;    def __getitem__(self, index):&#10;        episode_id, start_ts = self._locate_transition(index)&#10;        dataset_path = self.dataset_path_list[episode_id]&#10;        with h5py.File(dataset_path, 'r') as root:&#10;            try:  # some legacy data does not have this attribute&#10;                is_sim = root.attrs['sim']&#10;            except:&#10;                is_sim = False&#10;            compressed = root.attrs.get('compress', False)&#10;&#10;            action = root['/action'][()]&#10;            original_action_shape = action.shape&#10;            episode_len = original_action_shape[0]&#10;            # get observation at start_ts only&#10;            qpos = root['/observations/qpos'][start_ts]&#10;            qvel = root['/observations/qvel'][start_ts]&#10;            image_dict = dict()&#10;            for cam_name in self.camera_names:&#10;                image_dict[cam_name] = root[f'/observations/images/{cam_name}'][start_ts]&#10;&#10;            if compressed:&#10;                for cam_name in image_dict.keys():&#10;                    decompressed_image = cv2.imdecode(image_dict[cam_name], 1)&#10;                    image_dict[cam_name] = np.array(decompressed_image)&#10;&#10;            # get all actions after and including start_ts&#10;            if is_sim:&#10;                action = action[start_ts:]&#10;            else:&#10;                action = action[max(0, start_ts - 1):]  # hack, to make timesteps more aligned&#10;&#10;        # self.is_sim = is_sim&#10;&#10;        # --- 修改开始 ---&#10;        # 确保动作序列长度固定为 chunk_size&#10;        action_len = action.shape[0]&#10;        padded_action = np.zeros((self.chunk_size, original_action_shape[1]), dtype=np.float32)&#10;        # 截断或填充动作序列&#10;        copy_len = min(action_len, self.chunk_size)&#10;        padded_action[:copy_len] = action[:copy_len]&#10;&#10;        # 创建 is_pad 掩码&#10;        is_pad = np.zeros(self.chunk_size, dtype=bool)&#10;        is_pad[copy_len:] = True&#10;        # --- 修改结束 ---&#10;&#10;        # new axis for different cameras&#10;        all_cam_images = []&#10;        for cam_name in self.camera_names:&#10;            all_cam_images.append(image_dict[cam_name])&#10;        all_cam_images = np.stack(all_cam_images, axis=0)&#10;&#10;        # construct observations&#10;        image_data = torch.from_numpy(all_cam_images)&#10;        qpos_data = torch.from_numpy(qpos).float()&#10;        action_data = torch.from_numpy(padded_action).float()&#10;        is_pad = torch.from_numpy(is_pad).bool()&#10;&#10;        # channel last&#10;        image_data = torch.einsum('k h w c -&gt; k c h w', image_data)&#10;&#10;        # augmentation&#10;        if self.transformations is None:&#10;            print('Initializing transformations')&#10;            original_size = image_data.shape[2:]&#10;            ratio = 0.95&#10;            self.transformations = [&#10;                transforms.RandomCrop(size=[int(original_size[0] * ratio), int(original_size[1] * ratio)]),&#10;                transforms.Resize(original_size, antialias=True),&#10;                transforms.RandomRotation(degrees=[-5.0, 5.0], expand=False),&#10;                transforms.ColorJitter(brightness=0.3, contrast=0.4, saturation=0.5)  # , hue=0.08)&#10;            ]&#10;&#10;        if self.augment_images:&#10;            for transform in self.transformations:&#10;                image_data = transform(image_data)&#10;&#10;        # normalize image and change dtype to float&#10;        image_data = image_data / 255.0&#10;&#10;        if self.policy_class == 'Diffusion':&#10;            # normalize to [-1, 1]&#10;            action_data = ((action_data - self.norm_stats[&quot;action_min&quot;]) / (&#10;                        self.norm_stats[&quot;action_max&quot;] - self.norm_stats[&quot;action_min&quot;])) * 2 - 1&#10;        else:&#10;            # normalize to mean 0 std 1&#10;            action_data = (action_data - self.norm_stats[&quot;action_mean&quot;]) / self.norm_stats[&quot;action_std&quot;]&#10;&#10;        qpos_data = (qpos_data - self.norm_stats[&quot;qpos_mean&quot;]) / self.norm_stats[&quot;qpos_std&quot;]&#10;&#10;        return image_data, qpos_data, action_data, is_pad&#10;&#10;&#10;def get_norm_stats(dataset_path_list):&#10;    all_qpos_data = []&#10;    all_action_data = []&#10;    all_episode_len = []&#10;&#10;    for dataset_path in dataset_path_list:&#10;        try:&#10;            with h5py.File(dataset_path, 'r') as root:&#10;                qpos = root['/observations/qpos'][()]&#10;                qvel = root['/observations/qvel'][()]&#10;                if '/base_action' in root:&#10;                    base_action = root['/base_action'][()]&#10;                    base_action = preprocess_base_action(base_action)&#10;                    action = np.concatenate([root['/action'][()], base_action], axis=-1)&#10;                else:&#10;                    action = root['/action'][()]&#10;        except Exception as e:&#10;            print(f'Error loading {dataset_path} in get_norm_stats')&#10;            print(e)&#10;            quit()&#10;        all_qpos_data.append(torch.from_numpy(qpos))&#10;        all_action_data.append(torch.from_numpy(action))&#10;        all_episode_len.append(len(qpos))&#10;    all_qpos_data = torch.cat(all_qpos_data, dim=0)&#10;    all_action_data = torch.cat(all_action_data, dim=0)&#10;&#10;    # normalize action data&#10;    action_mean = all_action_data.mean(dim=[0]).float()&#10;    action_std = all_action_data.std(dim=[0]).float()&#10;    action_std = torch.clip(action_std, 1e-2, np.inf) # clipping&#10;&#10;    # normalize qpos data&#10;    qpos_mean = all_qpos_data.mean(dim=[0]).float()&#10;    qpos_std = all_qpos_data.std(dim=[0]).float()&#10;    qpos_std = torch.clip(qpos_std, 1e-2, np.inf) # clipping&#10;&#10;    action_min = all_action_data.min(dim=0).values.float()&#10;    action_max = all_action_data.max(dim=0).values.float()&#10;&#10;    eps = 0.0001&#10;    stats = {&quot;action_mean&quot;: action_mean.numpy(), &quot;action_std&quot;: action_std.numpy(),&#10;             &quot;action_min&quot;: action_min.numpy() - eps,&quot;action_max&quot;: action_max.numpy() + eps,&#10;             &quot;qpos_mean&quot;: qpos_mean.numpy(), &quot;qpos_std&quot;: qpos_std.numpy(),&#10;             &quot;example_qpos&quot;: qpos}&#10;&#10;    return stats, all_episode_len&#10;&#10;def find_all_hdf5(dataset_dir, skip_mirrored_data):&#10;    hdf5_files = []&#10;    for root, dirs, files in os.walk(dataset_dir):&#10;        for filename in fnmatch.filter(files, '*.hdf5'):&#10;            if 'features' in filename: continue&#10;            if skip_mirrored_data and 'mirror' in filename:&#10;                continue&#10;            hdf5_files.append(os.path.join(root, filename))&#10;    print(f'Found {len(hdf5_files)} hdf5 files')&#10;    return hdf5_files&#10;&#10;def BatchSampler(batch_size, episode_len_l, sample_weights):&#10;    sample_probs = np.array(sample_weights) / np.sum(sample_weights) if sample_weights is not None else None&#10;    sum_dataset_len_l = np.cumsum([0] + [np.sum(episode_len) for episode_len in episode_len_l])&#10;    while True:&#10;        batch = []&#10;        for _ in range(batch_size):&#10;            episode_idx = np.random.choice(len(episode_len_l), p=sample_probs)&#10;            step_idx = np.random.randint(sum_dataset_len_l[episode_idx], sum_dataset_len_l[episode_idx + 1])&#10;            batch.append(step_idx)&#10;        yield batch&#10;&#10;def load_data(dataset_dir_l, name_filter, camera_names, batch_size_train, batch_size_val, chunk_size, skip_mirrored_data=False,&#10;              load_pretrain=False, policy_class=None, stats_dir_l=None, sample_weights=None, train_ratio=0.99):&#10;&#10;    if type(dataset_dir_l) == str:&#10;        dataset_dir_l = [dataset_dir_l]&#10;    dataset_path_list_list = [find_all_hdf5(dataset_dir, skip_mirrored_data) for dataset_dir in dataset_dir_l]&#10;    num_episodes_0 = len(dataset_path_list_list[0])&#10;    dataset_path_list = flatten_list(dataset_path_list_list)&#10;    dataset_path_list = [n for n in dataset_path_list if name_filter(n)]&#10;    num_episodes_l = [len(dataset_path_list) for dataset_path_list in dataset_path_list_list]&#10;    num_episodes_cumsum = np.cumsum(num_episodes_l)&#10;&#10;    # obtain train test split on dataset_dir_l[0]&#10;    shuffled_episode_ids_0 = np.random.permutation(num_episodes_0)&#10;    train_episode_ids_0 = shuffled_episode_ids_0[:int(train_ratio * num_episodes_0)]#####注意&#10;    val_episode_ids_0 = shuffled_episode_ids_0[int(train_ratio * num_episodes_0):]&#10;    train_episode_ids_l = [train_episode_ids_0] + [np.arange(num_episodes) + num_episodes_cumsum[idx] for idx, num_episodes in enumerate(num_episodes_l[1:])]&#10;    val_episode_ids_l = [val_episode_ids_0]&#10;    train_episode_ids = np.concatenate(train_episode_ids_l)&#10;    val_episode_ids = np.concatenate(val_episode_ids_l)&#10;    print(f'\n\nData from: {dataset_dir_l}\n- Train on {[len(x) for x in train_episode_ids_l]} episodes\n- Test on {[len(x) for x in val_episode_ids_l]} episodes\n\n')&#10;&#10;    # obtain normalization stats for qpos and action&#10;    # if load_pretrain:&#10;    #     with open(os.path.join('/home/zfu/interbotix_ws/src/act/ckpts/pretrain_all', 'dataset_stats.pkl'), 'rb') as f:&#10;    #         norm_stats = pickle.load(f)&#10;    #     print('Loaded pretrain dataset stats')&#10;    _, all_episode_len = get_norm_stats(dataset_path_list)&#10;    train_episode_len_l = [[all_episode_len[i] for i in train_episode_ids] for train_episode_ids in train_episode_ids_l]&#10;    val_episode_len_l = [[all_episode_len[i] for i in val_episode_ids] for val_episode_ids in val_episode_ids_l]&#10;    train_episode_len = flatten_list(train_episode_len_l)&#10;    val_episode_len = flatten_list(val_episode_len_l)&#10;    if stats_dir_l is None:&#10;        stats_dir_l = dataset_dir_l&#10;    elif type(stats_dir_l) == str:&#10;        stats_dir_l = [stats_dir_l]&#10;    norm_stats, _ = get_norm_stats(flatten_list([find_all_hdf5(stats_dir, skip_mirrored_data) for stats_dir in stats_dir_l]))&#10;    print(f'Norm stats from: {stats_dir_l}')&#10;&#10;    batch_sampler_train = BatchSampler(batch_size_train, train_episode_len_l, sample_weights)&#10;    batch_sampler_val = BatchSampler(batch_size_val, val_episode_len_l, None)&#10;    # print(f'train_episode_len: {train_episode_len}, val_episode_len: {val_episode_len}, train_episode_ids: {train_episode_ids}, val_episode_ids: {val_episode_ids}')&#10;&#10;    # construct dataset and dataloader&#10;    train_dataset = EpisodicDataset(dataset_path_list, camera_names, norm_stats, train_episode_ids, train_episode_len, chunk_size, policy_class)&#10;    val_dataset = EpisodicDataset(dataset_path_list, camera_names, norm_stats, val_episode_ids, val_episode_len, chunk_size, policy_class)&#10;&#10;    for data in train_dataset:&#10;        print(&quot;train_dataset_训练集张量&quot;, data[2].shape)&#10;        break&#10;    for data in val_dataset:&#10;        print(&quot;测试集是多少&quot;,data[2].shape)&#10;        break&#10;    # train_num_workers = (8 if os.getlogin() == 'zfu' else 16) if train_dataset.augment_images else 2&#10;    # val_num_workers = 8 if train_dataset.augment_images else 2&#10;    train_num_workers = 8&#10;    val_num_workers = 8&#10;    print(f'Augment images: {train_dataset.augment_images}, train_num_workers: {train_num_workers}, val_num_workers: {val_num_workers}')&#10;    train_dataloader = DataLoader(train_dataset, batch_sampler=batch_sampler_train, pin_memory=True, num_workers=train_num_workers, prefetch_factor=2)&#10;    val_dataloader = DataLoader(val_dataset, batch_sampler=batch_sampler_val, pin_memory=True, num_workers=val_num_workers, prefetch_factor=2)&#10;&#10;    return train_dataloader, val_dataloader, norm_stats, train_dataset.is_sim&#10;&#10;def calibrate_linear_vel(base_action, c=None):&#10;    if c is None:&#10;        c = 0.0 # 0.19&#10;    v = base_action[..., 0]&#10;    w = base_action[..., 1]&#10;    base_action = base_action.copy()&#10;    base_action[..., 0] = v - c * w&#10;    return base_action&#10;&#10;def smooth_base_action(base_action):&#10;    return np.stack([&#10;        np.convolve(base_action[:, i], np.ones(5)/5, mode='same') for i in range(base_action.shape[1])&#10;    ], axis=-1).astype(np.float32)&#10;&#10;def preprocess_base_action(base_action):&#10;    # base_action = calibrate_linear_vel(base_action)&#10;    base_action = smooth_base_action(base_action)&#10;&#10;    return base_action&#10;&#10;def postprocess_base_action(base_action):&#10;    linear_vel, angular_vel = base_action&#10;    linear_vel *= 1.0&#10;    angular_vel *= 1.0&#10;    # angular_vel = 0&#10;    # if np.abs(linear_vel) &lt; 0.05:&#10;    #     linear_vel = 0&#10;    return np.array([linear_vel, angular_vel])&#10;&#10;### env utils&#10;&#10;def sample_box_pose():&#10;    x_range = [0.0, 0.2]&#10;    y_range = [0.4, 0.6]&#10;    z_range = [0.05, 0.05]&#10;&#10;    ranges = np.vstack([x_range, y_range, z_range])&#10;    cube_position = np.random.uniform(ranges[:, 0], ranges[:, 1])&#10;&#10;    cube_quat = np.array([1, 0, 0, 0])&#10;    return np.concatenate([cube_position, cube_quat])&#10;&#10;def sample_insertion_pose():&#10;    # Peg&#10;    x_range = [0.1, 0.2]&#10;    y_range = [0.4, 0.6]&#10;    z_range = [0.05, 0.05]&#10;&#10;    ranges = np.vstack([x_range, y_range, z_range])&#10;    peg_position = np.random.uniform(ranges[:, 0], ranges[:, 1])&#10;&#10;    peg_quat = np.array([1, 0, 0, 0])&#10;    peg_pose = np.concatenate([peg_position, peg_quat])&#10;&#10;    # Socket&#10;    x_range = [-0.2, -0.1]&#10;    y_range = [0.4, 0.6]&#10;    z_range = [0.05, 0.05]&#10;&#10;    ranges = np.vstack([x_range, y_range, z_range])&#10;    socket_position = np.random.uniform(ranges[:, 0], ranges[:, 1])&#10;&#10;    socket_quat = np.array([1, 0, 0, 0])&#10;    socket_pose = np.concatenate([socket_position, socket_quat])&#10;&#10;    return peg_pose, socket_pose&#10;&#10;### helper functions&#10;&#10;def compute_dict_mean(epoch_dicts):&#10;    result = {k: None for k in epoch_dicts[0]}&#10;    num_items = len(epoch_dicts)&#10;    for k in result:&#10;        value_sum = 0&#10;        for epoch_dict in epoch_dicts:&#10;            value_sum += epoch_dict[k]&#10;        result[k] = value_sum / num_items&#10;    return result&#10;&#10;def detach_dict(d):&#10;    new_d = dict()&#10;    for k, v in d.items():&#10;        new_d[k] = v.detach()&#10;    return new_d&#10;&#10;def set_seed(seed):&#10;    torch.manual_seed(seed)&#10;    np.random.seed(seed)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>